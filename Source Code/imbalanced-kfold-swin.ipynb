{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11040779,"sourceType":"datasetVersion","datasetId":6877214},{"sourceId":11155876,"sourceType":"datasetVersion","datasetId":6960425}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom transformers import SwinModel, SwinConfig\n\n# Custom Dataset Class\nclass CovidPneumoniaDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = ['covid', 'pneumonia']\n        self.images = []\n        self.labels = []\n        \n        for label, class_name in enumerate(self.classes):\n            class_dir = os.path.join(root_dir, class_name)\n            if os.path.exists(class_dir):\n                for img_name in os.listdir(class_dir):\n                    img_path = os.path.join(class_dir, img_name)\n                    self.images.append(img_path)\n                    self.labels.append(label)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label = self.labels[idx]\n        \n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Custom Model with Swin Tiny Transformer\nclass SwinBinaryClassifier(nn.Module):\n    def __init__(self):\n        super(SwinBinaryClassifier, self).__init__()\n        self.swin = SwinModel.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n        self.head = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 2)\n        )\n    \n    def forward_features(self, x):\n        return self.swin(x).last_hidden_state[:, 0, :]\n    \n    def forward(self, x):\n        features = self.forward_features(x)\n        return self.head(features)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:12:32.250062Z","iopub.execute_input":"2025-04-19T12:12:32.250249Z","iopub.status.idle":"2025-04-19T12:12:55.325737Z","shell.execute_reply.started":"2025-04-19T12:12:32.250233Z","shell.execute_reply":"2025-04-19T12:12:55.324998Z"}},"outputs":[{"name":"stderr","text":"2025-04-19 12:12:44.481926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745064764.660214      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745064764.710553      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load full dataset\ndata_dir = \"/kaggle/input/covid-pneumonia-lus-images/covid_pneumonia\"  # Replace with your dataset path containing covid/ and pneumonia/ folders\nfull_dataset = CovidPneumoniaDataset(root_dir=data_dir, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:13:10.389391Z","iopub.execute_input":"2025-04-19T12:13:10.389668Z","iopub.status.idle":"2025-04-19T12:13:10.536016Z","shell.execute_reply.started":"2025-04-19T12:13:10.389646Z","shell.execute_reply":"2025-04-19T12:13:10.535498Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import random\n# Step 1: Count samples per class\nlabels = np.array(full_dataset.labels)  # Assuming labels are 0 (COVID) and 1 (pneumonia)\ncovid_indices = np.where(labels == 0)[0]  # Indices of COVID samples\npneumonia_indices = np.where(labels == 1)[0]  # Indices of pneumonia samples\nn_covid = len(covid_indices)\nn_pneumonia = len(pneumonia_indices)\n\nprint(f\"Original: COVID samples = {n_covid}, Pneumonia samples = {n_pneumonia}\")\n\n# Step 2: Calculate target number of pneumonia samples for 10:1 imbalance\ntarget_pneumonia = n_covid // 10  # Integer division to get ~1/10th of COVID samples\n\nif target_pneumonia == 0:\n    raise ValueError(\"Too few COVID samples to achieve 10:1 imbalance. Increase dataset size or adjust ratio.\")\n\n# Step 3: Undersample pneumonia to achieve 10:1 ratio\nif n_pneumonia > target_pneumonia:\n    # Randomly select target_pneumonia samples from pneumonia\n    pneumonia_indices = random.sample(list(pneumonia_indices), target_pneumonia)\nelse:\n    # If there are fewer pneumonia samples, use all of them and warn about imbalance\n    print(f\"Warning: Only {n_pneumonia} pneumonia samples available, less than {target_pneumonia} needed for 10:1 ratio.\")\n    target_pneumonia = n_pneumonia\n\n# Step 4: Combine indices for the imbalanced dataset\nselected_indices = list(covid_indices) + list(pneumonia_indices)\nrandom.shuffle(selected_indices)  # Shuffle to avoid ordering bias\nimbalanced_dataset = Subset(full_dataset, selected_indices)\n\n# Update labels for the imbalanced dataset\nimbalanced_labels = labels[selected_indices]\nprint(f\"Imbalanced: COVID samples = {len(covid_indices)}, Pneumonia samples = {len(pneumonia_indices)} (Ratio ~ {len(covid_indices)/len(pneumonia_indices):.2f}:1)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:13:12.809128Z","iopub.execute_input":"2025-04-19T12:13:12.809410Z","iopub.status.idle":"2025-04-19T12:13:12.817626Z","shell.execute_reply.started":"2025-04-19T12:13:12.809388Z","shell.execute_reply":"2025-04-19T12:13:12.816979Z"}},"outputs":[{"name":"stdout","text":"Original: COVID samples = 524, Pneumonia samples = 463\nImbalanced: COVID samples = 524, Pneumonia samples = 52 (Ratio ~ 10.08:1)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Perform train-test split\ntrain_idx, test_idx = train_test_split(\n    range(len(imbalanced_dataset)),\n    test_size=0.2,  # 20% for test set\n    stratify=imbalanced_labels,  # Maintain 10:1 class distribution\n    random_state=42\n)\n\ntrain_dataset = Subset(imbalanced_dataset, train_idx)\ntest_dataset = Subset(imbalanced_dataset, test_idx)\n\n# Cross-validation parameters\nn_splits = 5\nnum_epochs = 5\nfinal_num_epochs = 10\nbatch_size = 16\nkfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Training and validation loop for hyperparameter tuning\nfold_val_accs = []\nfold_val_f1s = []\n\nfor fold, (train_idx_cv, val_idx_cv) in enumerate(kfold.split(range(len(train_dataset)))):\n    print(f'\\nFold {fold + 1}/{n_splits}')\n    \n    # Create train and validation subsets from training data\n    train_subset = Subset(train_dataset, train_idx_cv)\n    val_subset = Subset(train_dataset, val_idx_cv)\n    \n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n    \n    model = SwinBinaryClassifier().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n    # criterion = nn.CrossEntropyLoss()\n    # Calculate class weights (inverse frequency or custom)\n    class_counts = np.bincount(imbalanced_labels)\n    class_weights = 1.0 / class_counts\n    class_weights = class_weights / class_weights.sum()  # Normalize\n    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n    \n    # Update criterion in both CV and final training\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        train_preds, train_labels = [], []\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_preds.extend(predicted.cpu().numpy())\n            train_labels.extend(labels.cpu().numpy())\n        \n        train_acc = accuracy_score(train_labels, train_preds) * 100\n        avg_loss = running_loss / len(train_loader)\n        \n        # Validation\n        model.eval()\n        val_preds, val_labels = [], []\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n                val_preds.extend(predicted.cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n        \n        val_acc = accuracy_score(val_labels, val_preds) * 100\n        val_f1 = f1_score(val_labels, val_preds, average='binary')\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n        \n        scheduler.step()\n    \n    fold_val_accs.append(val_acc)\n    fold_val_f1s.append(val_f1)\n\n# Cross-validation results\nprint('\\nCross-validation Results:')\nprint(f'Average Val Accuracy: {np.mean(fold_val_accs):.2f}% (±{np.std(fold_val_accs):.2f})')\nprint(f'Average Val F1 Score: {np.mean(fold_val_f1s):.4f} (±{np.std(fold_val_f1s):.4f})')\n\n# Train final model on full training set\nprint('\\nTraining final model on full training set...')\nfinal_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nfinal_model = SwinBinaryClassifier().to(device)\noptimizer = optim.Adam(final_model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\nfor epoch in range(final_num_epochs):\n    final_model.train()\n    running_loss = 0.0\n    train_preds, train_labels = [], []\n    \n    for images, labels in final_train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = final_model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_preds.extend(predicted.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n    \n    train_acc = accuracy_score(train_labels, train_preds) * 100\n    avg_loss = running_loss / len(final_train_loader)\n    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    scheduler.step()\n\n# Evaluate on test set\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nfinal_model.eval()\ntest_preds, test_labels = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = final_model(images)\n        _, predicted = torch.max(outputs, 1)\n        test_preds.extend(predicted.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\ntest_acc = accuracy_score(test_labels, test_preds) * 100\ntest_f1 = f1_score(test_labels, test_preds, average='binary')\ncm = confusion_matrix(test_labels, test_preds)\n\nprint('\\nFinal Test Set Results:')\nprint(f'Test Accuracy: {test_acc:.2f}%')\nprint(f'Test F1 Score: {test_f1:.4f}')\nprint('Confusion Matrix:')\nprint(cm)\nprint(\"Training and evaluation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:18:54.623718Z","iopub.execute_input":"2025-04-19T12:18:54.624436Z","iopub.status.idle":"2025-04-19T12:23:35.436652Z","shell.execute_reply.started":"2025-04-19T12:18:54.624411Z","shell.execute_reply":"2025-04-19T12:23:35.436011Z"}},"outputs":[{"name":"stdout","text":"\nFold 1/5\nEpoch 1/5:\nTrain Loss: 0.5753, Train Acc: 69.29%\nVal Acc: 91.30%, Val F1: 0.5556\nEpoch 2/5:\nTrain Loss: 0.2888, Train Acc: 91.03%\nVal Acc: 94.57%, Val F1: 0.6667\nEpoch 3/5:\nTrain Loss: 0.2194, Train Acc: 92.12%\nVal Acc: 94.57%, Val F1: 0.6667\nEpoch 4/5:\nTrain Loss: 0.1819, Train Acc: 91.85%\nVal Acc: 94.57%, Val F1: 0.6667\nEpoch 5/5:\nTrain Loss: 0.1750, Train Acc: 91.03%\nVal Acc: 94.57%, Val F1: 0.6667\n\nFold 2/5\nEpoch 1/5:\nTrain Loss: 0.5393, Train Acc: 83.42%\nVal Acc: 94.57%, Val F1: 0.7368\nEpoch 2/5:\nTrain Loss: 0.3152, Train Acc: 92.12%\nVal Acc: 88.04%, Val F1: 0.5926\nEpoch 3/5:\nTrain Loss: 0.2752, Train Acc: 89.40%\nVal Acc: 95.65%, Val F1: 0.8000\nEpoch 4/5:\nTrain Loss: 0.2070, Train Acc: 94.29%\nVal Acc: 94.57%, Val F1: 0.7619\nEpoch 5/5:\nTrain Loss: 0.2003, Train Acc: 93.48%\nVal Acc: 94.57%, Val F1: 0.7619\n\nFold 3/5\nEpoch 1/5:\nTrain Loss: 0.5788, Train Acc: 89.13%\nVal Acc: 93.48%, Val F1: 0.7500\nEpoch 2/5:\nTrain Loss: 0.3290, Train Acc: 90.49%\nVal Acc: 89.13%, Val F1: 0.6667\nEpoch 3/5:\nTrain Loss: 0.2800, Train Acc: 91.30%\nVal Acc: 85.87%, Val F1: 0.6061\nEpoch 4/5:\nTrain Loss: 0.2609, Train Acc: 89.13%\nVal Acc: 85.87%, Val F1: 0.6061\nEpoch 5/5:\nTrain Loss: 0.2184, Train Acc: 90.49%\nVal Acc: 85.87%, Val F1: 0.6061\n\nFold 4/5\nEpoch 1/5:\nTrain Loss: 0.5507, Train Acc: 85.60%\nVal Acc: 90.22%, Val F1: 0.6087\nEpoch 2/5:\nTrain Loss: 0.3069, Train Acc: 92.66%\nVal Acc: 88.04%, Val F1: 0.5600\nEpoch 3/5:\nTrain Loss: 0.2140, Train Acc: 93.75%\nVal Acc: 90.22%, Val F1: 0.6087\nEpoch 4/5:\nTrain Loss: 0.1871, Train Acc: 93.48%\nVal Acc: 90.22%, Val F1: 0.6087\nEpoch 5/5:\nTrain Loss: 0.1895, Train Acc: 93.48%\nVal Acc: 90.22%, Val F1: 0.6087\n\nFold 5/5\nEpoch 1/5:\nTrain Loss: 0.4835, Train Acc: 82.88%\nVal Acc: 90.22%, Val F1: 0.6400\nEpoch 2/5:\nTrain Loss: 0.3552, Train Acc: 88.59%\nVal Acc: 91.30%, Val F1: 0.6667\nEpoch 3/5:\nTrain Loss: 0.2588, Train Acc: 90.49%\nVal Acc: 93.48%, Val F1: 0.7273\nEpoch 4/5:\nTrain Loss: 0.2216, Train Acc: 91.85%\nVal Acc: 93.48%, Val F1: 0.7273\nEpoch 5/5:\nTrain Loss: 0.1789, Train Acc: 92.39%\nVal Acc: 93.48%, Val F1: 0.7273\n\nCross-validation Results:\nAverage Val Accuracy: 91.74% (±3.34)\nAverage Val F1 Score: 0.6741 (±0.0624)\n\nTraining final model on full training set...\nEpoch 1/5: Train Loss: 0.2845, Train Acc: 86.74%\nEpoch 2/5: Train Loss: 0.1576, Train Acc: 92.17%\nEpoch 3/5: Train Loss: 0.1128, Train Acc: 94.13%\nEpoch 4/5: Train Loss: 0.1002, Train Acc: 94.57%\nEpoch 5/5: Train Loss: 0.0970, Train Acc: 95.65%\nEpoch 6/5: Train Loss: 0.0977, Train Acc: 95.22%\nEpoch 7/5: Train Loss: 0.0937, Train Acc: 95.87%\nEpoch 8/5: Train Loss: 0.1003, Train Acc: 94.78%\nEpoch 9/5: Train Loss: 0.1028, Train Acc: 95.43%\nEpoch 10/5: Train Loss: 0.0929, Train Acc: 96.09%\n\nFinal Test Set Results:\nTest Accuracy: 97.41%\nTest F1 Score: 0.8235\nConfusion Matrix:\n[[106   0]\n [  3   7]]\nTraining and evaluation complete!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nlabels = ['covid', 'pneumonia']\nclass_report = classification_report(test_labels, test_preds, target_names=labels)\nprint(class_report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T07:12:06.229042Z","iopub.execute_input":"2025-04-18T07:12:06.230139Z","iopub.status.idle":"2025-04-18T07:12:06.245607Z","shell.execute_reply.started":"2025-04-18T07:12:06.230109Z","shell.execute_reply":"2025-04-18T07:12:06.244901Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       covid       0.92      1.00      0.96       106\n   pneumonia       1.00      0.10      0.18        10\n\n    accuracy                           0.92       116\n   macro avg       0.96      0.55      0.57       116\nweighted avg       0.93      0.92      0.89       116\n\n","output_type":"stream"}],"execution_count":7}]}