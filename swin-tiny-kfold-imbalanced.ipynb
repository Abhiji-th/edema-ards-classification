{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11040779,"sourceType":"datasetVersion","datasetId":6877214}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import SwinModel, SwinConfig\n\n# Custom Dataset Class\nclass CovidPneumoniaDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = ['covid', 'pneumonia']\n        self.images = []\n        self.labels = []\n        \n        # Load images and labels\n        for label, class_name in enumerate(self.classes):\n            class_dir = os.path.join(root_dir, class_name)\n            for img_name in os.listdir(class_dir):\n                img_path = os.path.join(class_dir, img_name)\n                self.images.append(img_path)\n                self.labels.append(label)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label = self.labels[idx]\n        \n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Custom Model with Swin Tiny Transformer\nclass SwinBinaryClassifier(nn.Module):\n    def __init__(self):\n        super(SwinBinaryClassifier, self).__init__()\n        # Load pretrained Swin Tiny\n        self.swin = SwinModel.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n        # Custom head for binary classification\n        self.head = nn.Sequential(\n            nn.Linear(768, 256),  # Swin Tiny output is 768\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 2)  # 2 classes: covid and pneumonia\n        )\n    \n    def forward_features(self, x):\n        return self.swin(x).last_hidden_state[:, 0, :]  # Get CLS token\n    \n    def forward(self, x):\n        features = self.forward_features(x)\n        return self.head(features)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\n# Load dataset\ndata_dir = \"/kaggle/input/covid-pneumonia-lus-images/covid_pneumonia\"  # Replace with your dataset path\ndataset = CovidPneumoniaDataset(root_dir=data_dir, transform=transform)\n\n# Cross-validation parameters\nn_splits = 5\nnum_epochs = 5\nbatch_size = 16\nkfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Training and validation loop\nfold_train_accs = []\nfold_val_accs = []\nfold_val_f1s = []\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n    print(f'\\nFold {fold + 1}/{n_splits}')\n    \n    # Create data loaders\n    train_subset = torch.utils.data.Subset(dataset, train_idx)\n    val_subset = torch.utils.data.Subset(dataset, val_idx)\n    \n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n    \n    # Initialize model, optimizer, and loss\n    model = SwinBinaryClassifier().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        train_preds, train_labels = [], []\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_preds.extend(predicted.cpu().numpy())\n            train_labels.extend(labels.cpu().numpy())\n        \n        train_acc = accuracy_score(train_labels, train_preds) * 100\n        avg_loss = running_loss / len(train_loader)\n        \n        # Validation\n        model.eval()\n        val_preds, val_labels = [], []\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n                val_preds.extend(predicted.cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n        \n        val_acc = accuracy_score(val_labels, val_preds) * 100\n        val_f1 = f1_score(val_labels, val_preds, average='binary')\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n        \n        scheduler.step()\n    \n    fold_train_accs.append(train_acc)\n    fold_val_accs.append(val_acc)\n    fold_val_f1s.append(val_f1)\n\n# Print final results\nprint('\\nCross-validation Results:')\nprint(f'Average Train Accuracy: {np.mean(fold_train_accs):.2f}% (±{np.std(fold_train_accs):.2f})')\nprint(f'Average Val Accuracy: {np.mean(fold_val_accs):.2f}% (±{np.std(fold_val_accs):.2f})')\nprint(f'Average Val F1 Score: {np.mean(fold_val_f1s):.4f} (±{np.std(fold_val_f1s):.4f})')\n\nprint(\"Training complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:34:15.977125Z","iopub.execute_input":"2025-04-05T09:34:15.977305Z","iopub.status.idle":"2025-04-05T09:41:27.513634Z","shell.execute_reply.started":"2025-04-05T09:34:15.977286Z","shell.execute_reply":"2025-04-05T09:41:27.512730Z"}},"outputs":[{"name":"stdout","text":"\nFold 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a367ccc229a14340b11e027ac01aa0de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/113M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"418733ddf88a46ebb189a011567aa574"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/5:\nTrain Loss: 0.5683, Train Acc: 71.61%\nVal Acc: 95.45%, Val F1: 0.9508\nEpoch 2/5:\nTrain Loss: 0.1453, Train Acc: 96.20%\nVal Acc: 98.99%, Val F1: 0.9888\nEpoch 3/5:\nTrain Loss: 0.0492, Train Acc: 98.99%\nVal Acc: 99.49%, Val F1: 0.9944\nEpoch 4/5:\nTrain Loss: 0.0418, Train Acc: 99.49%\nVal Acc: 99.49%, Val F1: 0.9944\nEpoch 5/5:\nTrain Loss: 0.0417, Train Acc: 99.62%\nVal Acc: 99.49%, Val F1: 0.9944\n\nFold 2/5\nEpoch 1/5:\nTrain Loss: 0.5323, Train Acc: 76.55%\nVal Acc: 94.95%, Val F1: 0.9444\nEpoch 2/5:\nTrain Loss: 0.1116, Train Acc: 95.44%\nVal Acc: 100.00%, Val F1: 1.0000\nEpoch 3/5:\nTrain Loss: 0.0620, Train Acc: 98.61%\nVal Acc: 99.49%, Val F1: 0.9942\nEpoch 4/5:\nTrain Loss: 0.0456, Train Acc: 99.37%\nVal Acc: 99.49%, Val F1: 0.9942\nEpoch 5/5:\nTrain Loss: 0.0489, Train Acc: 98.99%\nVal Acc: 99.49%, Val F1: 0.9942\n\nFold 3/5\nEpoch 1/5:\nTrain Loss: 0.3873, Train Acc: 84.05%\nVal Acc: 94.92%, Val F1: 0.9474\nEpoch 2/5:\nTrain Loss: 0.1317, Train Acc: 94.81%\nVal Acc: 93.40%, Val F1: 0.9326\nEpoch 3/5:\nTrain Loss: 0.0770, Train Acc: 96.08%\nVal Acc: 96.45%, Val F1: 0.9626\nEpoch 4/5:\nTrain Loss: 0.0718, Train Acc: 96.33%\nVal Acc: 96.45%, Val F1: 0.9626\nEpoch 5/5:\nTrain Loss: 0.0591, Train Acc: 97.09%\nVal Acc: 96.45%, Val F1: 0.9626\n\nFold 4/5\nEpoch 1/5:\nTrain Loss: 0.4221, Train Acc: 80.51%\nVal Acc: 98.98%, Val F1: 0.9907\nEpoch 2/5:\nTrain Loss: 0.1196, Train Acc: 95.70%\nVal Acc: 100.00%, Val F1: 1.0000\nEpoch 3/5:\nTrain Loss: 0.0470, Train Acc: 99.11%\nVal Acc: 100.00%, Val F1: 1.0000\nEpoch 4/5:\nTrain Loss: 0.0355, Train Acc: 99.49%\nVal Acc: 100.00%, Val F1: 1.0000\nEpoch 5/5:\nTrain Loss: 0.0406, Train Acc: 99.49%\nVal Acc: 100.00%, Val F1: 1.0000\n\nFold 5/5\nEpoch 1/5:\nTrain Loss: 0.3678, Train Acc: 86.33%\nVal Acc: 92.89%, Val F1: 0.9239\nEpoch 2/5:\nTrain Loss: 0.1373, Train Acc: 94.81%\nVal Acc: 95.43%, Val F1: 0.9529\nEpoch 3/5:\nTrain Loss: 0.0653, Train Acc: 96.58%\nVal Acc: 95.94%, Val F1: 0.9579\nEpoch 4/5:\nTrain Loss: 0.0562, Train Acc: 96.96%\nVal Acc: 95.94%, Val F1: 0.9579\nEpoch 5/5:\nTrain Loss: 0.0593, Train Acc: 96.46%\nVal Acc: 95.94%, Val F1: 0.9579\n\nCross-validation Results:\nAverage Train Accuracy: 98.33% (±1.30)\nAverage Val Accuracy: 98.28% (±1.72)\nAverage Val F1 Score: 0.9818 (±0.0178)\nTraining complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom transformers import SwinModel, SwinConfig\n\n# Custom Dataset Class\nclass CovidPneumoniaDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = ['covid', 'pneumonia']\n        self.images = []\n        self.labels = []\n        \n        for label, class_name in enumerate(self.classes):\n            class_dir = os.path.join(root_dir, class_name)\n            if os.path.exists(class_dir):\n                for img_name in os.listdir(class_dir):\n                    img_path = os.path.join(class_dir, img_name)\n                    self.images.append(img_path)\n                    self.labels.append(label)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label = self.labels[idx]\n        \n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Custom Model with Swin Tiny Transformer\nclass SwinBinaryClassifier(nn.Module):\n    def __init__(self):\n        super(SwinBinaryClassifier, self).__init__()\n        self.swin = SwinModel.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n        self.head = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 2)\n        )\n    \n    def forward_features(self, x):\n        return self.swin(x).last_hidden_state[:, 0, :]\n    \n    def forward(self, x):\n        features = self.forward_features(x)\n        return self.head(features)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\n# Load full dataset\ndata_dir = \"/kaggle/input/covid-pneumonia-lus-images/covid_pneumonia\"  # Replace with your dataset path containing covid/ and pneumonia/ folders\nfull_dataset = CovidPneumoniaDataset(root_dir=data_dir, transform=transform)\n\n# Perform train-test split\ntrain_idx, test_idx = train_test_split(\n    range(len(full_dataset)),\n    test_size=0.2,  # 20% for test set\n    stratify=full_dataset.labels,  # Maintain class distribution\n    random_state=42\n)\n\ntrain_dataset = Subset(full_dataset, train_idx)\ntest_dataset = Subset(full_dataset, test_idx)\n\n# Cross-validation parameters\nn_splits = 5\nnum_epochs = 5\nbatch_size = 16\nkfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Training and validation loop for hyperparameter tuning\nfold_val_accs = []\nfold_val_f1s = []\n\nfor fold, (train_idx_cv, val_idx_cv) in enumerate(kfold.split(range(len(train_dataset)))):\n    print(f'\\nFold {fold + 1}/{n_splits}')\n    \n    # Create train and validation subsets from training data\n    train_subset = Subset(train_dataset, train_idx_cv)\n    val_subset = Subset(train_dataset, val_idx_cv)\n    \n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n    \n    model = SwinBinaryClassifier().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        train_preds, train_labels = [], []\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_preds.extend(predicted.cpu().numpy())\n            train_labels.extend(labels.cpu().numpy())\n        \n        train_acc = accuracy_score(train_labels, train_preds) * 100\n        avg_loss = running_loss / len(train_loader)\n        \n        # Validation\n        model.eval()\n        val_preds, val_labels = [], []\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n                val_preds.extend(predicted.cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n        \n        val_acc = accuracy_score(val_labels, val_preds) * 100\n        val_f1 = f1_score(val_labels, val_preds, average='binary')\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n        \n        scheduler.step()\n    \n    fold_val_accs.append(val_acc)\n    fold_val_f1s.append(val_f1)\n\n# Cross-validation results\nprint('\\nCross-validation Results:')\nprint(f'Average Val Accuracy: {np.mean(fold_val_accs):.2f}% (±{np.std(fold_val_accs):.2f})')\nprint(f'Average Val F1 Score: {np.mean(fold_val_f1s):.4f} (±{np.std(fold_val_f1s):.4f})')\n\n# Train final model on full training set\nprint('\\nTraining final model on full training set...')\nfinal_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nfinal_model = SwinBinaryClassifier().to(device)\noptimizer = optim.Adam(final_model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\nfor epoch in range(num_epochs):\n    final_model.train()\n    running_loss = 0.0\n    train_preds, train_labels = [], []\n    \n    for images, labels in final_train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = final_model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_preds.extend(predicted.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n    \n    train_acc = accuracy_score(train_labels, train_preds) * 100\n    avg_loss = running_loss / len(final_train_loader)\n    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    scheduler.step()\n\n# Evaluate on test set\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nfinal_model.eval()\ntest_preds, test_labels = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = final_model(images)\n        _, predicted = torch.max(outputs, 1)\n        test_preds.extend(predicted.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\ntest_acc = accuracy_score(test_labels, test_preds) * 100\ntest_f1 = f1_score(test_labels, test_preds, average='binary')\ncm = confusion_matrix(test_labels, test_preds)\n\nprint('\\nFinal Test Set Results:')\nprint(f'Test Accuracy: {test_acc:.2f}%')\nprint(f'Test F1 Score: {test_f1:.4f}')\nprint('Confusion Matrix:')\nprint(cm)\nprint(\"Training and evaluation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:51:03.360691Z","iopub.execute_input":"2025-04-05T09:51:03.361068Z","iopub.status.idle":"2025-04-05T09:57:50.803598Z","shell.execute_reply.started":"2025-04-05T09:51:03.361036Z","shell.execute_reply":"2025-04-05T09:57:50.802849Z"}},"outputs":[{"name":"stdout","text":"\nFold 1/5\nEpoch 1/5:\nTrain Loss: 0.4667, Train Acc: 78.92%\nVal Acc: 87.34%, Val F1: 0.8750\nEpoch 2/5:\nTrain Loss: 0.1601, Train Acc: 93.34%\nVal Acc: 93.67%, Val F1: 0.9390\nEpoch 3/5:\nTrain Loss: 0.1230, Train Acc: 94.93%\nVal Acc: 94.94%, Val F1: 0.9506\nEpoch 4/5:\nTrain Loss: 0.0766, Train Acc: 96.83%\nVal Acc: 95.57%, Val F1: 0.9565\nEpoch 5/5:\nTrain Loss: 0.0736, Train Acc: 97.31%\nVal Acc: 95.57%, Val F1: 0.9565\n\nFold 2/5\nEpoch 1/5:\nTrain Loss: 0.5038, Train Acc: 79.56%\nVal Acc: 88.61%, Val F1: 0.8929\nEpoch 2/5:\nTrain Loss: 0.2047, Train Acc: 90.33%\nVal Acc: 97.47%, Val F1: 0.9747\nEpoch 3/5:\nTrain Loss: 0.1051, Train Acc: 96.99%\nVal Acc: 98.73%, Val F1: 0.9873\nEpoch 4/5:\nTrain Loss: 0.1005, Train Acc: 97.46%\nVal Acc: 100.00%, Val F1: 1.0000\nEpoch 5/5:\nTrain Loss: 0.0808, Train Acc: 98.10%\nVal Acc: 100.00%, Val F1: 1.0000\n\nFold 3/5\nEpoch 1/5:\nTrain Loss: 0.4161, Train Acc: 83.52%\nVal Acc: 95.57%, Val F1: 0.9560\nEpoch 2/5:\nTrain Loss: 0.1705, Train Acc: 93.50%\nVal Acc: 98.10%, Val F1: 0.9806\nEpoch 3/5:\nTrain Loss: 0.0871, Train Acc: 96.67%\nVal Acc: 97.47%, Val F1: 0.9744\nEpoch 4/5:\nTrain Loss: 0.0749, Train Acc: 97.15%\nVal Acc: 98.10%, Val F1: 0.9806\nEpoch 5/5:\nTrain Loss: 0.0730, Train Acc: 96.99%\nVal Acc: 98.10%, Val F1: 0.9806\n\nFold 4/5\nEpoch 1/5:\nTrain Loss: 0.4106, Train Acc: 82.41%\nVal Acc: 91.77%, Val F1: 0.9023\nEpoch 2/5:\nTrain Loss: 0.1518, Train Acc: 94.93%\nVal Acc: 86.08%, Val F1: 0.8136\nEpoch 3/5:\nTrain Loss: 0.1411, Train Acc: 94.29%\nVal Acc: 98.10%, Val F1: 0.9781\nEpoch 4/5:\nTrain Loss: 0.0825, Train Acc: 97.94%\nVal Acc: 98.73%, Val F1: 0.9853\nEpoch 5/5:\nTrain Loss: 0.0708, Train Acc: 97.94%\nVal Acc: 98.73%, Val F1: 0.9853\n\nFold 5/5\nEpoch 1/5:\nTrain Loss: 0.4358, Train Acc: 82.44%\nVal Acc: 93.63%, Val F1: 0.9324\nEpoch 2/5:\nTrain Loss: 0.1776, Train Acc: 94.15%\nVal Acc: 94.90%, Val F1: 0.9444\nEpoch 3/5:\nTrain Loss: 0.1111, Train Acc: 95.41%\nVal Acc: 94.27%, Val F1: 0.9388\nEpoch 4/5:\nTrain Loss: 0.1033, Train Acc: 95.57%\nVal Acc: 94.90%, Val F1: 0.9444\nEpoch 5/5:\nTrain Loss: 0.0842, Train Acc: 96.36%\nVal Acc: 94.90%, Val F1: 0.9444\n\nCross-validation Results:\nAverage Val Accuracy: 97.46% (±1.93)\nAverage Val F1 Score: 0.9734 (±0.0201)\n\nTraining final model on full training set...\nEpoch 1/5: Train Loss: 0.3777, Train Acc: 85.30%\nEpoch 2/5: Train Loss: 0.1200, Train Acc: 95.82%\nEpoch 3/5: Train Loss: 0.0768, Train Acc: 96.45%\nEpoch 4/5: Train Loss: 0.0630, Train Acc: 96.58%\nEpoch 5/5: Train Loss: 0.0701, Train Acc: 96.58%\n\nFinal Test Set Results:\nTest Accuracy: 95.96%\nTest F1 Score: 0.9588\nConfusion Matrix:\n[[97  8]\n [ 0 93]]\nTraining and evaluation complete!\n","output_type":"stream"}],"execution_count":2}]}